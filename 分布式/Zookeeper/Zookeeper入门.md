# Zookeeper入门

## 概述

ZooKeeper 是 Apache 软件基金会的一个软件项目，它为大型分布式计算提供开源的**分布式配置服务、同步服务和命名注册**；ZooKeeper 的架构通过冗余服务实现高可用性（CP）。

Zookeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用

> Zookeeper 可以理解为一个分布式协调服务，本质：小型的文件存储系统 + 监听通知机制；如存储配置信息和监听配置的变更

## 数据结构

zookeeper本身是一个**树形目录服务**（名称空间），非常类似于**标准文件系统**，key-value 的形式存储。名称 key 由斜线 / 分割的一系列路径元素，zookeeper 名称空间中的每个节点都是由一个路径来标识的。

![image-20231126204148216](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231126204148216.png)

> 每个路径下的节点key(完整路径，名称)是唯一的，即同一级节点 key 名称是唯一的
>
> 每个节点中存储了节点value和对应的状态属性（多个）

## 节点类型

- PERSISTENT：持久节点，创建成功以后不会自动删除 (默认）
- PERSISTENT_SEQUENTIAL：持久顺序节点，创建时zookeeper 会在路径上加上序号作为后缀，**非常适合用于分布式锁、分布式选举等场景**，创建时添加 -s 参数
- EPHEMERAL：**临时节点(不可在拥有子节点)**，跟连接会话绑定，临时节点会在客户端会话断开后由zk服务端自动删除。**适用于心跳，服务发现等场景**，创建时添加 -e 参数
- EPHEMERAL_SEQUENTIAL：临时顺序节点(不可在拥有子节点)，与持久顺序节点类似，不同之处在于EPHEMERAL_SEQUENTIAL是临时的，会在会话断开后删除，创建时添加 -e -s 参数
- CONTAINER：容器节点，当子节点都被删除后，Container 也随即删除，创建时添加 -c 参数
- PERSISTENT_WITH_TTL：TTL节点，客户端断开连接后不会自动删除Znode，如果该Znode没有子Znode且在给定TTL时间内无修改，该Znode将会被删除；单位是毫秒；创建时添加 -t 参数

## 常见的客户端操作命令

| 命令             | 描述                   | 用法示例                                                    | 说明                                                         |
| ---------------- | ---------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| ls               | 查看某个路径下目录列表 | ls [-s] [-w] [-R] path                                      | path：代表路径，完整路径<br/>-s：返回状态信息<br/>-w：监听节点变化<br/>-R:递归查看某路径下目录列表 |
| create           | 创建节点并赋值         | create [-s] [-e] [-c] [-t ttl] path [data] [acl]            | [-s] [-e]：-s 和 -e 都是可选的，-s 代表顺序节点， -e 代表临时节点，注意其中 -s 和 -e 可以同时使用的，并且临时节点不能再创建子节点<br/>path：指定要创建节点的路径，比如 /runoob<br/>data：要在此节点存储的数据<br/>acl：访问权限相关，默认是 world，相当于全世界都能访问 |
| set              | 修改节点存储的数据     | set [-s] [-v version] path data                             | path：节点路径。<br/>data：需要存储的数据。<br/>[version]：可选项，版本号(可用作乐观锁) |
| get              | 获取节点数据和状态信息 | get [-s] [-w] path                                          | -s：返回结果带上状态信息<br/>-w:返回数据并对对节点进行事件监听(触发事件后停止监听) |
| stat             | 获取节点数据和状态信息 | stat [-w] path                                              | path：代表路径<br/>-w:对节点进行事件监听                     |
| delete/deleteall | 删除某节点             | delete [-v version] path<br/>deleteall path [-b batch size] | 如果某节点不为空，则不能用delete命令删除                     |

## 数据模型 znode 结构

![image-20231126212029617](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231126212029617.png)

| 状态属性       | 描述                                                         |
| -------------- | ------------------------------------------------------------ |
| cZxid          | 创建节点时的事务ID                                           |
| ctime          | 创建节点时的时间                                             |
| mZxid          | 最后修改节点时的事务ID                                       |
| mtime          | 最后修改节点时的时间                                         |
| pZxid          | 表示该节点的子节点列表最后一次修改的事务ID，添加子节点或删除子节点就会影响子节点列表，但是修改子节点的数据内容则不影响该ID（注意，只有子节点列表变更了才会变更pzxid，子节点内容变更不会影响pzxid） |
| cversion       | 子节点版本号，子节点每次修改版本号加1                        |
| dataversion    | 数据版本号，数据每次修改该版本号加1                          |
| aclversion     | 权限版本号，权限每次修改该版本号加1                          |
| ephemeralOwner | 创建该临时节点的会话的sessionID。（**如果该节点是持久节点，那么这个属性值为0）** |
| dataLength     | 该节点的数据长度                                             |
| numChildren    | 该节点拥有子节点的数量（只统计直接子节点的数量）             |

## 分布式锁-zk实现

1. 普通实现：注册临时节点，谁注册成功谁获取锁，其他监听该节点的删除事件，一旦被删除，通知其他客户端，再次重复该流程；

> 此为最简单的实现，但容易引发一些问题：大量的客户端同时注册节点，zookeeper压力过大

2. 临时顺序节点
   1. 所有服务要获取锁时都去zookeeper中注册一个**临时顺序节点**，并将基本信息写入临时节点
   2. 所有服务获取节点列表并**判断自己的节点是否是最小的那个**，谁最小谁就获取了锁
   3. 未获取锁的客户端添加**对前一个节点删除事件的监听**
   4. 锁释放/持有锁的客户端宕机 后，节点被删除
   5. 下一个节点的客户端收到通知，重复上述流程

### 读写锁

**共享锁**：又称读锁。如果事务T1对数据对象O1加上了共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放

**排它锁**：又称写锁或独占锁。如果事务T1对数据对象O1加上了排他锁，那么在整个加锁期间，只允许事务T1对O1进行读取或更新操作，其他任务事务都不能对这个数据对象进行任何操作，直到T1释放了排他锁



可以将临时有序节点分为**读锁**节点和**写锁**节点

➢ 对于读锁节点而言，其只需要关心**前一个写锁节点**的释放。如果前一个写锁释放了，则多个读锁节点对应的线程可以并发地读取数据

> 如何理解`前一个写锁节点`: 读锁节点前面可能有多个读锁节点，但是它不关心，它只关心前面的写锁节点

➢对于写锁节点而言，其**只需要关心前一个节点的释放**，而不需要关心前一个节点是写锁节点还是读锁节点。因为为了保证有序性，**写操作必须要等待前面的读操作或者写操作执行完成**

> 读写互斥

## 分布式锁-Curator实现

![image-20231127203659849](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231127203659849.png)

➢InterProcessMutex：分布式可重入排它锁（可重入可以借助LocalMap存计数器）

➢InterProcessSemaphoreMutex：分布式排它锁

➢InterProcessMultiLock：将多个锁作为单个实体管理的容器

➢InterProcessReadWriteLock：分布式读写锁

## ZAB协议

### Paxos算法模型

Paxos算法是Lamport宗师提出的一种基于消息传递的分布式一致性算法，解决的问题正是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。

该算法定义了三个角色：

- **Proposer**: 提出提案 (Proposal)
- **Acceptor**：参与决策，回应Proposers的提案
- **Learner**：不参与决策，从Proposers/Acceptors学习最新达成一致的提案（Value）

> 提案遵循少数服从多数的原则，过半原则

### ZAB协议

Zab借鉴了Paxos算法，**它是特别为Zookeeper设计的支持崩溃恢复的原子广播协议**，全称是 **Zookeeper Atomic Broadcast** （[Zookeeper](https://so.csdn.net/so/search?q=Zookeeper&spm=1001.2101.3001.7020)原子广播）

基于该协议，Zookeeper实现了一种主备模型（即Leader和Follower模型）的系统架构来保证集群中各个副本之间数据的一致性

> 这里的主备系统架构模型，就是指只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader客户端将数据同步到其他Followers节点



![ZooKeeper Service](./Zookeeper%E5%85%A5%E9%97%A8.assets/zkservice.jpg)



在ZAB协议中，有三个主要的角色：

1. **领导者（Leader）**：集群中唯一的写请求处理者，能够发起投票（投票也是为了进行写请求）

2. **跟随者（Follower）**：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给领导者。在选举过程中会参与投票，有选举权和被选举权

3. **观察者（Observer）**：和Follower类似，就是没有选举权和被选举权

### ZAB协议模式

**崩溃恢复**:一旦Leader服务器出现崩溃或者由于网络原因导致 Leader 服务器失去了与过半 Follower的联系那么就会进入崩溃恢复模式。在 Zab 协议中，为了保证程序的正确运行，整个恢复过程结束后需要选举出个新的Leader服务器，并与过半follower完成数据同步，然后进入到消息广播模式

> 如果当前集群中没有Leader角色，那么就会进入崩溃恢复模式，基于选举算法选出Leader

**原子广播**: 这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且Leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。需要注意的是，Zab 提交事务并不像 2PC一样需要全部Follower都Ack，只需要得到quorum (超过半数的节点)的Ack就可以

## Leader选举

zookeeper 的 leader 选举存在两个阶段，一个是服务器启动时 leader 选举，另一个是运行过程中 leader 服务器宕机

选举的重要参数：

- **服务器 ID(myid)**：编号越大在选举算法中权重越大
- **事务 ID(zxid)**：值越大说明数据越新，权重越大
- **逻辑时钟(epoch-logicalclock)**：同一轮投票过程中的逻辑时钟值是相同的，每投完一次值会增加

选举状态：

- **LOOKING**: 竞选状态
- **FOLLOWING**: 随从状态，同步 leader 状态，参与投票
- **OBSERVING**: 观察状态，同步 leader 状态，不参与投票
- **LEADING**: 领导者状态

选举过程：

![img](./Zookeeper%E5%85%A5%E9%97%A8.assets/vote-01.png)

## 集群数据读写

### 读请求

当 Client 向 Zookeeper 发出读请求时，无论是 Leader 还是 Follower，都直接返回查询结果

> 一般来讲，Client是不会直连Leader节点的

![image-20231127213501863](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231127213501863.png)

### 写请求—Leader

Client 向Leader 发出写请求，Leader 将数据写入到本节点，并将数据发送到所有的 Follower 节点，等待 Follower 节点返回，当 Leader 接收到一半以上节点(包含自己)返回写成功的信息之后，返回写入成功消息给 Client

![image-20231127213651295](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231127213651295.png)

### 写请求-Follower

Client 向 Follower 发出写请求, Follower 节点将请求转发给 Leader,Leader 将数据写入到本节点，并将数据发送到所有的 Follower 节点,等待 Follower 节点返回,当 Leader 接收到一半以上节点(包含自己)返回写成功的信息之后，返回写入成功消息给原来的 Follower ,原来的 Follower 返回写入成功消息给Client

![image-20231127213738081](./Zookeeper%E5%85%A5%E9%97%A8.assets/image-20231127213738081.png)
